<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data manipulation | Mike Nguyen</title>
    <link>https://mikenguyen.netlify.app/tag/data-manipulation/</link>
      <atom:link href="https://mikenguyen.netlify.app/tag/data-manipulation/index.xml" rel="self" type="application/rss+xml" />
    <description>data manipulation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Mike Nguyen 2024</copyright><lastBuildDate>Sat, 20 Feb 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mikenguyen.netlify.app/media/social_sharing_image.png</url>
      <title>data manipulation</title>
      <link>https://mikenguyen.netlify.app/tag/data-manipulation/</link>
    </image>
    
    <item>
      <title>fix for &#34;cannot allocate vector of size&#34;</title>
      <link>https://mikenguyen.netlify.app/post/fix-for-cannot-allocate-vector-of-size/</link>
      <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://mikenguyen.netlify.app/post/fix-for-cannot-allocate-vector-of-size/</guid>
      <description>
&lt;script src=&#34;https://mikenguyen.netlify.app/post/fix-for-cannot-allocate-vector-of-size/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;More package author’s introduction, please access this &lt;a href=&#34;https://cran.r-project.org/web/packages/disk.frame/readme/README.html&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instead of loading everything at once into your RAM, you divide your data into chunks.
To quote author of the &lt;code&gt;disk.frame&lt;/code&gt; package: “we go from”R can only deal with data that fits in RAM&#34; to “R can deal with any data that fits on disk”.&#34; While &lt;code&gt;data.frame&lt;/code&gt; uses in-RAM to process, &lt;code&gt;disk.frame&lt;/code&gt; uses hard drive to store and process data.&lt;code&gt;disk.frame&lt;/code&gt; also allows parallel processing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;disk.frame&amp;quot;) 
# setup_disk.frame() # sets up background workers equal to the number of CPU c res setup_disk.frame(workers =\ 2) \# or you number of workers options(future.globals.maxSize = \Inf) # large dataset can be transferred between sessions
# attr(data.df, &amp;quot;path&amp;quot;) # path to where the disk.frame is 


# to convert data.frame to a     disk.frame
data.df &amp;lt;- as.disk.frame(original_data_frame)

# to convert one large CSV
# takes care of splitting large CSV into smaller ones 
diskf &amp;lt;- disk.frame::csv_to_disk.frame(path_to_csv_file) # you can also specify,outdir = , overwrite = T.     

# to convert multiple CSV
multiple_CSV = c(path_to_csv_file1,path_to_csv_file2)
diskf = disk.frame::csv_to_disk.frame(multiple_CSV)

# for faster performance, specify which column to manipulate
result  = df %&amp;gt;% 
  srep(c(&amp;quot;column1&amp;quot;,&amp;quot;column2&amp;quot;)) %&amp;gt;%
  dplyr::filter()&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Connect WRDS in R</title>
      <link>https://mikenguyen.netlify.app/post/connect-wrds-in-r/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mikenguyen.netlify.app/post/connect-wrds-in-r/</guid>
      <description>


&lt;p&gt;Connect from R to Wharton Research Data Services&lt;/p&gt;
&lt;p&gt;to set up connection from R to WRDS (&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/pages/support/programming-wrds/programming-r/r-from-your-computer/&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RPostgres)
library(tidyverse)


# I&amp;#39;ve set up wrds connection before hand. 
# Please use your username and password here.
wrds &amp;lt;- dbConnect(
  Postgres(),
  host = &amp;#39;wrds-pgdata.wharton.upenn.edu&amp;#39;,
  port = 9737,
  dbname = &amp;#39;wrds&amp;#39;,
  sslmode = &amp;#39;require&amp;#39;,
  user = Sys.getenv(&amp;quot;wrds_user&amp;quot;),
  pass = Sys.getenv(&amp;quot;wrds_pass&amp;quot;)
)
knitr::opts_chunk$set(message = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;Check variables (column headers) in COMP ANNUAL FUNDAMENTAL&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- dbSendQuery(
  wrds,
  &amp;quot;select column_name
                   from information_schema.columns
                   where table_schema=&amp;#39;compa&amp;#39;
                   and table_name=&amp;#39;funda&amp;#39;
                   order by column_name&amp;quot;
)
# data &amp;lt;- dbFetch(res, n=-1) 
data &amp;lt;- dbFetch(res, n = 10)
dbClearResult(res) # closes the connection
head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   column_name
## 1       acchg
## 2        acco
## 3       accrt
## 4     acctchg
## 5     acctstd
## 6        acdo&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;All data libraries available at WRDS&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- dbSendQuery(wrds, &amp;quot;select distinct table_schema
                   from information_schema.tables
                   order by table_schema&amp;quot;)
all_lib &amp;lt;- dbFetch(res, n=-1)
dbClearResult(res)
head(all_lib)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           table_schema
## 1                  aha
## 2     aha_hcris_3years
## 3 aha_hcris_3years_old
## 4     aha_hcris_recent
## 5 aha_hcris_recent_old
## 6 aha_it_survey_3years&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;datasets within a given library&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- dbSendQuery(wrds, &amp;quot;select distinct table_name
                   from information_schema.columns
                   where table_schema=&amp;#39;comp_na_daily_all&amp;#39;
                   order by table_name&amp;quot;)
df_in_lib &amp;lt;- dbFetch(res, n=-1)
dbClearResult(res)
head(df_in_lib)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      table_name
## 1      aco_amda
## 2      aco_imda
## 3   aco_indfnta
## 4   aco_indfntq
## 5 aco_indfntytd
## 6    aco_indsta&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;variables within a given dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- dbSendQuery(wrds, &amp;quot;select column_name
                   from information_schema.columns
                   where table_schema=&amp;#39;comp_na_daily_all&amp;#39;
                   and table_name=&amp;#39;funda&amp;#39;
                   order by column_name&amp;quot;)
variables &amp;lt;- dbFetch(res, n=-1)
dbClearResult(res)
head(variables)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   column_name
## 1       acchg
## 2        acco
## 3       accrt
## 4     acctchg
## 5     acctstd
## 6        acdo&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;specific-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specific Example&lt;/h2&gt;
&lt;p&gt;Get advertising and R&amp;amp;D data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# from comp_na_daily_all.funda
res &amp;lt;- dbSendQuery(wrds, &amp;quot;select gvkey, fyear, xad, xrd
                   from comp_na_daily_all.funda
                   where fyear between 2000 and 2010 
                     and xad is not null 
                     and xrd is not null&amp;quot;)

# res &amp;lt;- dbSendQuery(wrds, &amp;quot;select gvkey, fyear
#                    from comp_na_daily_all.funda
#                    where fyear between 2000 and 2020&amp;quot;)

data &amp;lt;- dbFetch(res, n=10) |&amp;gt; 
# data &amp;lt;- dbFetch(res, n=-1) |&amp;gt; 
    unique()
dbClearResult(res)
head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    gvkey fyear     xad     xrd
## 1 001050  2000   0.188   0.140
## 2 001084  2000   1.711   0.036
## 3 001104  2000   3.170   0.046
## 4 001111  2000  16.500  41.396
## 5 001117  2000   0.161   1.175
## 6 001161  2000 148.000 641.799&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get industry data&lt;/p&gt;
&lt;p&gt;In WRDS, the industry classifications visible on the web version differ from those in the code version. This discrepancy arises because the web dataset undergoes (behind the scene) a merging process where different variables are combined to produce the final table. Consequently, in the library dataset &lt;code&gt;comp_na_daily_all.funda&lt;/code&gt;, key classifications such as &lt;code&gt;gind&lt;/code&gt;, &lt;code&gt;gsubind&lt;/code&gt;, &lt;code&gt;naics&lt;/code&gt;, and &lt;code&gt;sic&lt;/code&gt; are absent. Instead, it includes naics historical and sic historical, which often contain incomplete data. To obtain a complete view, users must merge this dataset with the &lt;a href=&#34;https://wrds-www.wharton.upenn.edu/data-dictionary/comp_na_daily_all/names/&#34;&gt;industry dataset&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get industry data
res &amp;lt;- dbSendQuery(wrds,
                   &amp;quot;select gvkey, gind, gsubind, naics, sic
                   from comp_na_daily_all.names&amp;quot;)
ind &amp;lt;- dbFetch(res, n = -1)
dbClearResult(res)
head(ind)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    gvkey   gind  gsubind  naics  sic
## 1 001000   &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt; 3089
## 2 001001 253010 25301040    722 5812
## 3 001002   &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt; 3825
## 4 001003 255040 25504040 442110 5712
## 5 001004 201010 20101010 423860 5080
## 6 001005   &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt; 3724&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data |&amp;gt; 
    left_join(ind)
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    gvkey fyear     xad     xrd   gind  gsubind  naics  sic
## 1 001050  2000   0.188   0.140 202010 20201050 333413 3564
## 2 001084  2000   1.711   0.036 502030 50203010 519290 7370
## 3 001104  2000   3.170   0.046 202010 20201060 332215 3420
## 4 001111  2000  16.500  41.396 451030 45103030 511210 7372
## 5 001117  2000   0.161   1.175 452010 45201020 334220 3663
## 6 001161  2000 148.000 641.799 453010 45301020 334413 3674&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linking Financial Databases (CRSP and Compustat)</title>
      <link>https://mikenguyen.netlify.app/post/link_crsp_compustat/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mikenguyen.netlify.app/post/link_crsp_compustat/</guid>
      <description>
&lt;link href=&#34;https://mikenguyen.netlify.app/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://mikenguyen.netlify.app/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Information can be found in &lt;a href=&#34;https://www.otago.ac.nz/library/pdf/CRSPCompustatguide09.pdf&#34;&gt;CRSP/COMPUSTAT MERGED DATABASE GUIDE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Change Identifiers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ticker&lt;/strong&gt;: can be reassign to another company - abbreviation used to uniquely identify publicly-traded shares of a stock&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUSIP&lt;/strong&gt;: A company can have multiple CUSIPS due to structural changes. - nine-character code assigned by the CUSIP Service Bureau to identify various securities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Permanent Identifiers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CRSP (maintained by Chicago BOoth CRSP)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PERMCO&lt;/strong&gt;: is a unique permanent company level identifier (even under name change)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PERMNO&lt;/strong&gt;: is a unique stock (share class) level identifier. One company can have multiple share classes. (1 PERMNO -&amp;gt; 1 PERMCO, 1 -&amp;gt; multiple PERMNOs)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Compustat-Capital IQ (maintained by S&amp;amp;P)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GVKEY&lt;/strong&gt;: is a unique number assigned to each company.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;CRSP&lt;/th&gt;
&lt;th&gt;COMPUSTAT&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Time since&lt;/td&gt;
&lt;td&gt;1925&lt;/td&gt;
&lt;td&gt;1950&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Companies&lt;/td&gt;
&lt;td&gt;listed in U.S. Exchange&lt;/td&gt;
&lt;td&gt;U.S. + Canada&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Report&lt;/td&gt;
&lt;td&gt;Daily and Monthly&lt;/td&gt;
&lt;td&gt;Quarterly and Annually&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Apache Arrow</title>
      <link>https://mikenguyen.netlify.app/post/apache-arrow/</link>
      <pubDate>Sun, 06 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mikenguyen.netlify.app/post/apache-arrow/</guid>
      <description>
&lt;link href=&#34;index_files/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;more information can be found in &lt;a href=&#34;https://ursalabs.org/&#34;&gt;URSA Labs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This example is from &lt;a href=&#34;https://ursalabs.org/arrow-r-nightly/articles/dataset.html&#34;&gt;Arrow Vignettes&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;arrow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;arrow&lt;/h1&gt;
&lt;p&gt;best when working with big data&lt;/p&gt;
&lt;div id=&#34;prep&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prep&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;arrow&amp;quot;, warn.conflicts = FALSE)
library(&amp;quot;dplyr&amp;quot;, warn.conflicts = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;check if S3 support is included.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arrow::arrow_with_s3()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If TRUE, sync data locally import from &lt;a href=&#34;https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page&#34;&gt;https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arrow::copy_files(&amp;quot;s3://ursa-labs-taxi-data&amp;quot;, &amp;quot;nyc-taxi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;since the data is in Parquet format, we use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ds &amp;lt;- open_dataset(&amp;quot;nyc-taxi&amp;quot;, partitioning = c(&amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then you can start using data set as usual&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ds&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
